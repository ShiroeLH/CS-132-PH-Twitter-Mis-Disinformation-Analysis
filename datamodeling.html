<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Stat & ML</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Nav -->
	
			<nav id="nav">
				<ul class="container">
					<li><a href="index.html">Home</a></li>
					<li><a href="index.html#overview">Overview</a></li>
					<li><a href="index.html#data">Data</a></li>
					<li><a href="index.html#methods">Methods</a></li>
					<li><a href="dataexploration.html">Exploration</a></li>
					<li><a href="#top">Stat & ML</a></li>
					<li><a href="index.html#results">Result</a></li>
					<li><a href="index.html#conclusion">Conclusion</a></li>
					<li><a href="index.html#poster">Poster</a></li>
					<li><a href="index.html#team">Team</a></li>

				</ul>
			</nav>
			

		<!-- Home -->
			<article id="top" class="wrapper style1">
				<div class="container">
					<div class="row">
						
					</div>
					<div class="row">
						<div class="col-12-medium">
							<header>
								<h1><img class= "logo centered mx-auto" src="images/logo.png" alt=""/>'s Stat Analysis and Data Modeling</h1>
								<p> Now, let's unveil the insights with Statistical Analysis using <strong><em><a href="#stat-test" style="text-decoration: none;">Chi-Square Test</a></em></strong>, and Machine Learning Techniques using <strong><em><a href="#mach-lrn" style="text-decoration: none;">Naive Bayes Classifier</a></em></strong>, and <strong><em><a href="#mach-lrn" style="text-decoration: none;">Long Short-Term Memory Networks</a></em></strong></p>
							</header>
						</div>
					</div>
					<footer>
						<a href="https://github.com/ShiroeLH/CS-132-PH-Twitter-Mis-Disinformation-Analysis/tree/main/analysis" class="button large scrolly  justify-content-center" target="_blank">Take a look at our Data Modeling code</a>
					</footer>
				</div>
			</article>

		<!-- STAT-TEST -->
			<article id="stat-test" class="wrapper style2">
				<div class="container">
					<header>
						<h1>Statistical Analysis using Chi-Square Test</h1>
						<p>Chi-square Test was used to analyze the association between red tag tweets and the mention of the University of the Philippines (UP)</p>
					</header>

					<div class="q-col">
						<h3>Hypothesis</h3>
						<div class="l">
							<b> Null Hypothesis (H0): </b>
							<br> No association between red tag tweets and the 
							mention of UP
						</div>
						<div class="l">
						<b> Alternative Hypothesis (H1):</b> 
						<br> UP is the dominating target of red tag tweets
						</div>
					</div>

					<section class="box style5">
						<div class="row">
							<div class="col-4 col-6-medium col-12-small">
								<a href="#" class="image mem-photo"><img src="images/pic01.jpg" alt="" /></a>
								<h3>Perform the Appropriate Statistical Test</h3>
								<p></p>
							</div>
							<div class="col-8 col-6-medium col-12-small">
								<p class = "description">
									To analyze the association between different categories of tweets, we used the chi-square test. Specifically, we employed the chi-square test available in the Scipy stats library, using the (<code>scipy.stats.chisquare</code>)function. This test allowed us to examine whether the observed frequencies of the different tweet categories significantly deviate from the expected frequencies, assuming that the categories are equally likely.
								</p>
								<p class = "description">We manually tagged the tweets into different categories: <br>1. those mentioning UP, <br>2. those mentioning a school other than UP, <br>3. those mentioning both UP and another school, and <br>4. those not mentioning any school names.
								</p>
							</div>	
						</div>
					</section>

					<section class="box style5">
						<div class="row">
							<div class="col-4 col-6-medium col-12-small">
								<a href="#" class="image mem-photo"><img src="images/pic01.jpg" alt="" /></a>
								<h3>Identify and Validate Assumptions</h3>
								<p></p>
							</div>
							<div class="col-8 col-6-medium col-12-small">
								<p class = "description">To determine the association between red tag tweets and the mention of UP, we utilized the Chi-Square test, which is a statistical test suitable for analyzing categorical data and assessing if the observed frequencies significantly deviate from the expected frequencies.
								</p>
								<p class = "description">It is important to note that the Chi-Square test relies on certain assumptions. These assumptions include:
									<br>1. All expected counts are at least 5 for <code>scipy.stats.chisquare</code> to be valid.
									<br> 2.	Individual observations are independent, and the population should be at least 10 times as large as the sample.
								</p>
								<p class="description">These assumptions are crucial in ensuring the reliability and validity of the Chi-Square test results.</p>

								<p class="description">The specific counts for the categories in our analysis are as follows: <br>
								1. Tweets mentioning UP: 87<br>
								2. Tweets mentioning a school other than UP: 9<br>
								3. Tweets mentioning both UP and another school: 22<br>
								4. Tweets not mentioning any school names: 50<br>
								These counts are used to determine the observed frequencies in the test. Since the tweets with mentions of both UP and other schools are counted independently, the categories are considered independent. The sample size analyzed consists of 168 tweets, which is most likely more than 10 times larger than the total number of tweets, related to the study, on Twitter.</p>	
							</div>	
						</div>
					</section>
					
					<img src="images/visualization/bargraph_3_observed_freq.png" alt="" />
					<section class="box style5">
						<div class="row">
							<div class="col-4 col-6-medium col-12-small">
								<a href="#" class="image mem-photo"><img src="images/pic01.jpg" alt="" /></a>
								<h3>Test Statistics and P-Values</h3>
								<p></p>
							</div>
							<div class="col-8 col-6-medium col-12-small">
								<p class = "description">After applying the chi-square test to our data, we obtained the following results:
								</p>								
								<p class = "description"><strong>Chi-square statistic</strong>
									: 85.19<br><strong>p-value</strong>
									: 2.36e-18<br><strong>Degrees of Freedom</strong>
									: 3<br><strong>Alpha (significance level)</strong>
									: 0.05
								</p>
							</div>	
						</div>
					</section>

					<section class="box style5">
						<div class="row">
							<div class="col-4 col-6-medium col-12-small">
								<a href="#" class="image mem-photo"><img src="images/pic01.jpg" alt="" /></a>
								<h3>Interpretation and Conclusion</h3>
								<p></p>
							</div>
							<div class="col-8 col-6-medium col-12-small">
								<p class = "description">To interpret the results, we compared the p-value with the significance level. If the p-value is less than the significance level, we reject the null hypothesis. Conversely, if the p-value is greater than or equal to the significance level, we fail to reject the null hypothesis.
								</p>
								<p class = "description">The chi-square statistic is a measure that helps assess the degree of association between categorical variables. In our analysis, the chi-square statistic of 85.19 indicates a significant deviation from the expected frequencies.  A higher chi-square statistic indicates a larger difference between the observed and expected frequencies which was set as equallit likely based on the implementation of <code>scipy.stats.chisquare</code>.
								</p>
								<p class = "description">Considering our results, the calculated p-value of 2.36e-18 is significantly lower than our chosen significance level of 0.05. Therefore, we reject the null hypothesis, which stated no association between red tag tweets and the mention of UP. The evidence provided by the chi-square test suggests a strong and significant association between the target of red tag tweets and the mention of UP.
								</p>
								<p class = "description">In summary, the obtained chi-square statistic of 85.19 and the very small p-value of 2.36e-18 provide strong evidence against the null hypothesis. Based on these findings, we have compelling evidence to support the claim that <strong>UP is the dominating target of red tag tweets</strong>. The chi-square analysis demonstrates a strong statistical association between the target of red tag tweets and the mention of UP, reinforcing the hypothesis that UP is specifically targeted in these instances.
								</p>
							</div>	
						</div>
					</section>
				</div>
			</article>

			<!-- MACHINE-LEARNING -->
			<article id="stat-test" class="wrapper style2">
				<div class="container">
					<header>
						<h1>Machine Learning using LSTM and Naive Bayes</h1>
						<p>Classification models were used to differentiate between red-tagging tweets and non-red tag related tweets that mention universities</p>
					</header>

					<div class="q-col">
						<h3>Procedures pre-Training</h3>
						<div class="l">
							<b> Python Modules: </b>
							<br> We used <i>pandas</i> and <i>numpy</i> for data manipulation, <i>nltk</i> for NLP, <i>matplotlib</i>, <i>seaborn</i>, and <i>plotly</i> for data visualization, <i>sklearn</i> (Multinomial Naive Bayes) and <i>keras</i> (LSTM) for training the model.
						</div>
						<div class="l">
							<b> Setting the Random Seed:</b> 
							<br> to make the output of the models more consistent with the given inputs. Note that since the models are affected by random seeds, we can set the seeds to a constant number (Do note however that this was <i>avoided for this project</i>). 
						</div>
						<div class="l">
							<b> Loading the Datasets:</b> 
							<br> The datasets found in `/analysis/Dataset - Group 32.xlsx` folder were loaded using <i>openpyxl</i>. The datasets comprise collected tweets from twitter and its translation to English which contain both red-tagging and non-red-tagging tweets.<br>
							<br> The whole dataset is split into 3 parts: <i>training set</i> and <i>test set</i> which in total contains <b>132 tweets</b>. The training set is used to train the model and the test set is used to test the model. The training set is <b>70% of the whole dataset and the test set is 30% of the whole dataset</b>. The datasets are split using <i>sklearn</i>.
						</div>
					</div>

					<br><br>
					<header>
						<h3>Naive Bayes</h3>
						<p>The Naive Bayes classifier is a simple probabilistic machine learning algorithm based on Bayes' theorem. It is called "naive" because it makes a strong assumption of independence among the features, assuming that the presence or absence of one feature does not affect the presence or absence of any other feature.</p>
					</header>
					<div class="r-question">
						<section class="box style5">
							<div class="row">
								<div class="col-4 col-6-medium col-12-small">
									<a href="#" class="image mem-photo"><img src="images/pic02.jpg" alt="" /></a>
									<h3>Benefits and Disadvantages</h3>
									<p></p>
								</div>
								<div class="col-8 col-6-medium col-12-small">
									
									<p class = "description">
										The Naive Bayes classifier calculates the probability of a given data instance belonging to a particular class based on the probabilities of the features. Here are some <b>benefits</b> of the Naive Bayes classifier: <br>
										<br>1. Simplicity: Naive Bayes is a straightforward algorithm that is relatively easy to understand and implement. 
										<br>2. Efficiency: Naive Bayes can be trained quickly and efficiently since it computes probabilities directly from the training data. 
										<br>3. Scalability: Due to its simplicity and efficiency, Naive Bayes can handle large datasets with high-dimensional feature spaces.
										<br>4. Robustness: Naive Bayes is robust to irrelevant features<br>

										<br>While the Naive Bayes classifier has several benefits, it also has some <b>limitations and disadvantages</b>. Here are some of the main drawbacks of the Naive Bayes classifier:<br>
										<br>1. Assumption of Feature Independence: The Naive Bayes classifier assumes that all features are independent of each other.
										<br>2. Sensitivity to Input Data: Naive Bayes can be sensitive to the quality and representativeness of the training data.  
										<br>3. Inability to Handle Missing Data: Naive Bayes does not handle missing values as it relies on complete data instances.
										<br>4. Limited Expressiveness: Due to its assumption of feature independence, Naive Bayes may struggle to capture complex relationships.
									</p>
								</div>	
							</div>
						</section>

						<section class="box style5">
							<div class="row">
								<div class="col-4 col-6-medium col-12-small">
									<a href="#" class="image mem-photo"><img src="images/pic02.jpg" alt="" /></a>
									<h3>Preparing the data</h3>
									<p></p>
								</div>
								<div class="col-8 col-6-medium col-12-small">
									<p class="description">
										In order to use the data for predictive modeling, we first prepare it by 'TfidfVectorizer'. The “Term Frequency  Inverse Document” Frequency method:<br>
										<br>(a) Term Frequency: summarizes how often a given word appears within a document
										<br>(b) Inverse Document Frequency: downscales words that appear a lot across documents.<br>

										<br>The `TfidfVectorizer` method is used to both the `text_train` and `text_test` data to tokenize, and learn the vocabulary,inverse document frequency weightings, and give a higher weight to words that occur less frequently.


										<br><br><b>Reference</b>: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/
									</p>
									
								</div>	
							</div>
						</section>

						<section class="box style5">
							<div class="row">
								<div class="col-4 col-6-medium col-12-small">
									<a href="#" class="image mem-photo"><img src="images/pic02.jpg" alt="" /></a>
									<h3>Training the data</h3>
									<p></p>
								</div>
								<div class="col-8 col-6-medium col-12-small">
									<p class="description">
										We could then train the Naive Bayes Model using the Multinomial Naive Bayes classifier `MultinomialNB`. This method from `sklearn.naive_bayes` is suitable for the data case where classification is based on the discrete feature which is the word count using `TfidfVectorizer`.
										The Naive Bayes classifier is fit according to the train data's text `train_term` and its corresponding tag `tag_train`. Classification is then performed on both the <b>normalized train data</b> `train_term` and <b>normalized test data</b> `test_term` from the previous section.
									</p>
									
								</div>	
							</div>
						</section>

						<section class="box style5">
							<div class="row">
								<div class="col-4 col-6-medium col-12-small">
									<a href="#" class="image mem-photo"><img src="images/pic02.jpg" alt="" /></a>
									<h3>Results</h3>
									<p></p>
								</div>
								<div class="col-8 col-6-medium col-12-small">
									<p class="description">
										The `sklearn.metric`’s confusion matrix was used to evaluate the Naive Bayes model’s performance. This matrix helps in visualizing the count for the predicted labels, `predictions_train` and `predictions_test`, versus the true labels, `tag_train`
										and `tag_test` respectively. The confusion matrix can be interpreted as the number of true positive connotations in the top left, false negative in the top right, false positive in the bottom left, and true negative in the lower right. The `plot_confusion_matrix` also has a parameter for normalizing the confusion matrix. 
									</p>
									
									<img src="images/visualization/confusionmatrix_naivebayes.png" alt="" />

									<p class="description">
										The method `accuracy_score` from `sklearn.metrics` is then used to compute for the <b>F1 score of the model</b>. This is done by computing the subset accuracy wherein the set of labels predicted for a sample must exactly match the corresponding set of labels in `tag_train` in the case of the train data and `tag_test` in the case of the test data.

										<br><br><b>Test Accuracy:</b> 0.9782608695652174
									</p>
								</div>	
							</div>
						</section>
					</div>

					<br><br>
					<header>
						<h3>LTSM</h3>
						<p>
							LSTM stands for Long Short-Term Memory, which is a type of recurrent neural network (RNN) architecture. LSTM networks are designed to overcome the limitations of traditional RNNs in capturing and remembering long-term dependencies in sequential data.</p>
					</header>
					<div class="r-question">
						<section class="box style5">
							<div class="row">
								<div class="col-4 col-6-medium col-12-small">
									<a href="#" class="image mem-photo"><img src="images/pic03.jpg" alt="" /></a>
									<h3>Benefits and Disadvantages</h3>
									<p></p>
								</div>
								<div class="col-8 col-6-medium col-12-small">
									
									<p class = "description">
										LSTMs have gained popularity in various applications, particularly in natural language processing (NLP), speech recognition, machine translation, and time series analysis. <b>LSTMs offer several benefits for classification tasks</b>:<br>
										<br>1. Handling Long-Term Dependencies: LSTMs are specifically designed to capture and model long-term dependencies in sequential data.  
										<br>2. Modeling Sequential Patterns: LSTMs can learn complex sequential patterns and relationships in data. They can identify temporal dependencies and patterns that exist across multiple time steps, allowing them to capture intricate relationships in the input sequence.
										<br>3. Robust to Noisy Data: LSTMs can handle noisy or missing data within a sequence. The memory cells in LSTMs enable the network to selectively retain or forget information, allowing them to handle noisy or irrelevant elements in the sequence.<br>

										<br>While LSTM networks offer several advantages for classification tasks, <b></b>they also have certain disadvantages</b>:
										<br>1. Computational Complexity: LSTMs can be computationally expensive, especially when dealing with large datasets or complex architectures. 
										<br>2. Overfitting: LSTMs, like other deep learning models, are prone to overfitting, especially when the training data is limited or unbalanced. Overfitting occurs when the model becomes too specialized in the training data and fails to generalize well to unseen data.
										<br>3. Large Training Data Requirements: LSTMs often require a significant amount of training data to learn meaningful patterns and achieve good classification performance. <br>

										<br>It is good to state here that given that the dataset used for this project only contained <b>305</b> tweets in total, the LSTM model may not be able to perform well.
									</p>
								</div>	
							</div>
						</section>

						<section class="box style5">
							<div class="row">
								<div class="col-4 col-6-medium col-12-small">
									<a href="#" class="image mem-photo"><img src="images/pic03.jpg" alt="" /></a>
									<h3>Pre-processing</h3>
									<p></p>
								</div>
								<div class="col-8 col-6-medium col-12-small">
									
									<p class = "description">
										For the preprocessing phase, each text in the dataset is tokenized using the `tf.keras.preprocessing.text.Tokenizer` method with the maximum number of features retrieved set to 3000. The `fit_on_texts` method updates the vocabulary to be used by the model using the phrases found in the translated tweet column of the dataset. Then, the tokens are transformed into a sequence of integers using the `texts_to_sequences` method and subsequently padded to match different text lengths using the `pad_sequences` method.
									</p>
								</div>	
							</div>
						</section>

						<section class="box style5">
							<div class="row">
								<div class="col-4 col-6-medium col-12-small">
									<a href="#" class="image mem-photo"><img src="images/pic03.jpg" alt="" /></a>
									<h3>Building the Model</h3>
									<p></p>
								</div>
								<div class="col-8 col-6-medium col-12-small">
									
									<p class = "description">
										We can then build the model using the Keras library. The LSTM model is first initialized using the Keras' `Sequential` class followed by the addition of <b>3</b> layer instances to the layer stack:<br>
										<br>1. Embedding Layer - Takes in the integer tokens generated	 from the previous cell
										<br>2. LSTM Layer - A <b>16</b> unit layer that processes the output from the embedding layer and handles weight updates for each word in the vocabulary.
										<br>3. Dense Layer - Takes in the output from the LSTM layer and has a <b>1</b> output neuron with a sigmoid activation function.<br>

										<br>`<i>Binary_crossentropy</i>` loss, since the model classifies data through Binary (<b>0 or 1</b>), and `<i>Adam optimizer</i>`, an efficient weight optimization algorithm, are then used to train the model. In a general sense, optimizers update the model by modifying attributes of the models of a neural network (e.g. weights and learning rates). First note that stochastic gradient descent optimizes an objective function, which in this case is the model. In a sense, it is a probabilistic approximation of gradient descent. Each step in a <b>SGD</b> calculates the gradient for one observation picked at random instead of the whole dataset. Adam optimizer is a combination of two <b>SGD</b> methodologies.

										<code>
											<br><br><b>CODE SNIPPET:</b><br>
											embed_dim = 64<br>
											lstm_out = 16<br>
											model = Sequential()<br>
											model.add(Embedding(max_features, embed_dim, input_length=text.shape[1]))<br>
											model.add(LSTM(lstm_out))<br>
											model.add(Dense(1,activation='sigmoid'))<br>
											model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])<br>
											print(model.summary())<br>
										</code>
									</p>
								</div>	
							</div>
						</section>
						
						<section class="box style5">
							<div class="row">
								<div class="col-4 col-6-medium col-12-small">
									<a href="#" class="image mem-photo"><img src="images/pic03.jpg" alt="" /></a>
									<h3>Training the Model</h3>
									<p></p>
								</div>
								<div class="col-8 col-6-medium col-12-small">
									
									<p class = "description">
										We split the tag and text values and check the size of the testing and training data set.
										<code>
											<br><br><b>CODE SNIPPET:</b><br>
											classification = df['Classification'].values<br>
											text_train, text_test, tag_train, tag_test = train_test_split(text,classification, test_size = 0.3)<br>
											print(text_train.shape,tag_train.shape)<br>
											print(text_test.shape,tag_test.shape)<br><br>
										</code>

										We train with a batch size of $16$ and train for $6$ epoch. The performance of the model is measured on the validation dataset.
										<code>
											<br><br><b>CODE SNIPPET:</b><br>
											batch_size = 16<br>
											model.fit(text_train,<br>
											&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tag_train,<br>
											&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;epochs=6,<br>
											&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size=batch_size,<br>
											&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;validation_data=(text_test, tag_test),<br>
											&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;callbacks = [EarlyStopping(monitor='val_accuracy',<br>
											&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;min_delta=0.001,<br>
											&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;patience=10,<br>
											&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;verbose=1)]<br>
											)<br>
										</code>

									</p>
								</div>	
							</div>
						</section>

						<section class="box style5">
							<div class="row">
								<div class="col-4 col-6-medium col-12-small">
									<a href="#" class="image mem-photo"><img src="images/pic03.jpg" alt="" /></a>
									<h3>Results</h3>
									<p></p>
								</div>
								<div class="col-8 col-6-medium col-12-small">
									
									<p class = "description">
										To check the accuracy of predictions of the LSTM model, the treshhold of 0.5 (negative if less than 0.5) was set to determine the classification of the text. The accuracy of the model is <b>0.9565217391304348</b> and the loss is <b>0.2990</b>.<br>
										<code>
											<br>7/7 [==============================] - 1s 19ms/step
											<br>3/3 [==============================] - 0s 15ms/step
											<br>Train accuracy: 0.9906103286384976
											<br>Test accuracy: 0.9565217391304348
										</code>

										<br><br>Similar to the Naive Bayes model, a confusion matrix is used to evaluate the LSTM model's performance.<br><br>

										<img src="images/visualization/confusionmatrix_ltsm.png" alt="" />

									</p>
								</div>	
							</div>
						</section>
					</div>
			</article>

		<!-- MACHINE-LEARNING 
			<article id="mach-lrn" class="wrapper style3">
				<div class="container">
					<header>
						<h1>Machine Learning</h1>
						<p>Lorem Ipsum</p>
					</header>

				</div>
			</article>
		-->


		<!-- We'd like to hear from you -->
			<article id="contact" class="wrapper style4">
				<div class="container medium">
					<header> 
						<h2>We'd like to hear from you</h2>
					</header>
					</div>
					<div class="row">
						<div class="col-12">
							<form name="contact_form">
								<div class="row">
									<div class="col-6 col-12-small">
										<input type="text" name="name" id="name" placeholder="Name"/>
									</div>
									<div class="col-6 col-12-small">
										<input type="email" name="email" id="email" placeholder="Email"/>
									</div>
									<div class="col-12">
										<input type="text" name="subject" id="subject" placeholder="Subject"/>
									</div>
									<div class="col-12">
										<textarea name="message" id="message" placeholder="Message"></textarea>
									</div>
									<div class="col-12">
										<ul class="actions">
											<li><button type="button" onclick="sendEmail()">Send a Message</button></li>
											<li><button type="reset" value="Clear Form" class="alt">Clear Form</button></li>
										</ul>
									</div>
								</div>
							</form>
						</div>
						<div class="col-12">
							<hr />
							<h3>Find me on ...</h3>
							<ul class="social">
								<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon brands fa-dribbble"><span class="label">Dribbble</span></a></li>
								<li><a href="#" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
								<li><a href="#" class="icon brands fa-tumblr"><span class="label">Tumblr</span></a></li>
								<li><a href="#" class="icon brands fa-google-plus"><span class="label">Google+</span></a></li>
								<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
								<!--
								<li><a href="#" class="icon solid fa-rss"><span>RSS</span></a></li>
								<li><a href="#" class="icon brands fa-instagram"><span>Instagram</span></a></li>
								<li><a href="#" class="icon brands fa-foursquare"><span>Foursquare</span></a></li>
								<li><a href="#" class="icon brands fa-skype"><span>Skype</span></a></li>
								<li><a href="#" class="icon brands fa-soundcloud"><span>Soundcloud</span></a></li>
								<li><a href="#" class="icon brands fa-youtube"><span>YouTube</span></a></li>
								<li><a href="#" class="icon brands fa-blogger"><span>Blogger</span></a></li>
								<li><a href="#" class="icon brands fa-flickr"><span>Flickr</span></a></li>
								<li><a href="#" class="icon brands fa-vimeo"><span>Vimeo</span></a></li>
								-->
							</ul>
							<hr />
						</div>
					</div>
					<footer>
						<ul id="copyright">
							<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>
				</div>
			</article>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="https://smtpjs.com/v3/smtp.js"></script>
			<script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script>

			<script>
				
				function sendEmail(){
					const form = document.getElementById("contact_form");
					name = document.getElementById("name").value;
					email = document.getElementById("email").value;
					subject = document.getElementById("subject").value;
					msg = document.getElementById("message").value;

					console.log(name,email,subject,msg)

					Email.send({
						SecureToken : "29b05827-fffb-43d0-9f97-a4042e25d0f2",
						To : 'redelosreyes3@up.edu.ph,aealoveros@up.edu.ph,jsdoros1@up.edu.ph',
						From : 'cs132contactus@gmail.com',
						Subject : [subject,' from ',email].join(''),
						Body : msg
					}).then(
					message => alert(message)
					);

				};

				


			</script>

	</body>
</html>
